---
title: "06 - Information Criteria"
author: "TJ Mahr"
date: "June 29, 2016"
output: github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "#>", collapse = TRUE)
```

## Preamble 

This is notebook of code I wrote while reading Chapter 6 of [_Statistical
Rethinking_](http://xcelab.net/rm/statistical-rethinking/). I don't use the
author's [rethinking package](https://github.com/rmcelreath/rethinking) or
any of its helper functions. I instead used [RStanARM](http://mc-stan.org/
interfaces/rstanarm) to fit the Bayesian models and [Loo](http://mc-stan.org/
interfaces/loo.html) for model comparisons, because I'm trying to stay in that
ecosystem. As a result, my models have different priors and different parameter
estimates.

```{r packages}
# devtools::install_github("rmcelreath/rethinking")
library("dplyr", warn.conflicts = FALSE)
library("rstanarm")
library("purrr")
library("tidyr")
library("ggplot2")

# I don't load the rethinking package, but I use data from it. Log version.
rethinking_info <- packageDescription(
  pkg = "rethinking", 
  fields = c("Version", "GithubUsername", "GithubRepo", 
             "GithubRef", "GithubSHA1"))
str(rethinking_info, give.attr = FALSE)
```


## Prepare the data

Get the primate milk data. The example models will predict the kilocalories
per gram of milk using the mass of the mother (log kg) and the proportion of
the brain that is neocortex. These data are from Hinde and Milligan (2011),
according to `?milk`.

```{r}
data(milk, package = "rethinking")
d <- milk %>% 
  as_data_frame %>% 
  filter(!is.na(neocortex.perc)) %>% 
  mutate(neocortex = neocortex.perc / 100,
         log_mass = log(mass)) %>% 
  select(clade, species, kcal.per.g, mass, log_mass, neocortex)
d %>% knitr::kable(digits = 3)
```

## Fit the models

Fit four different models that we will compare later.

```{r, message}
# Intercept only
m1 <- stan_glm(
  formula = kcal.per.g ~ 1,
  data = d,
  family = gaussian(),
  prior = normal(0, 1),
  prior_intercept = normal(.5, .5),
  prior_ops = prior_options(prior_scale_for_dispersion = .25)
)

# One predictor
m2 <- update(m1, . ~ neocortex)
m3 <- update(m1, . ~ log_mass)

# Two predictors
m4 <- update(m1, . ~ neocortex + log_mass)
```

Compare posterior to prior in each model.
 
```{r posterior-vs-prior}
# We want to have the same color-parameter mapping across plots, otherwise the
# sigma term will have three different colors. Manually set colors, using the D3
# color palette
d3_colors <- c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", 
               "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf")
p1 <- posterior_vs_prior(m1) + scale_color_manual(values = d3_colors[c(1, 4)])
p2 <- posterior_vs_prior(m2) + scale_color_manual(values = d3_colors[c(1, 3:4)])
p3 <- posterior_vs_prior(m3) + scale_color_manual(values = d3_colors[c(1:2, 4)])
p4 <- posterior_vs_prior(m4) + scale_color_manual(values = d3_colors[c(1:4)])
cowplot::plot_grid(p1, p2, p3, p4)
```




## Manual AIC calculation

To figure out what the information criteria are measuring, I want to calculate
them by hand.

The AIC works on classical models, so I wrote a helper function to convert
an RStanArm glm model into a classical glm model.

```{r}
# Refit a stanreg model using glm. Assumes no other glm arguments are used 
# besides formula, data and family--i.e., this is doesn't work for all glm
# models.
as_glm <- function(stan_model) {
  glm(stan_model$formula, data = stan_model$data, family = stan_model$family)
}

glm_m4 <- as_glm(m4)
```

Plus, another helper to get the maximum likelihood estimate of the model's sigma
term.

```{r}
# Get the maximum likelihood estimate of sigma from a model
# http://stats.stackexchange.com/a/73245
# https://stat.ethz.ch/pipermail/r-help/2003-June/035518.html
sigma_ml <- function(m) {
  squared_errors <- residuals(m) ^ 2
  sqrt(mean(squared_errors))
}
```

According to our model, each observation falls on a normal curve with a mean
equal to fitted value, and a standard deviation equal to model sigma. The
density of the fitted-value curve at the observed value is the likelihood of the
data.

To compute the likelihood of each observation, we find the density of
observations on their fitted-value curves. These are three such densities. (Code
omitted because it's hairy base-graphics code built from the example code
in `?dnorm`.)

```{r likelihood curves, echo = FALSE}
# Here are three likelihoods from the model.
plot(function(x) dnorm(x, glm_m4$fitted.values[1], sigma_ml(glm_m4)), 
     xlab = "Observations", ylab = "Likelihood", col = "grey75")

xs <- seq(.01, .99, .01)
for (i in c(1, 4, 6)) {
  density <- dnorm(d$kcal.per.g[i], glm_m4$fitted.values[i], sigma_ml(glm_m4))
  lines(xs, dnorm(xs, glm_m4$fitted.values[i], sigma_ml(glm_m4)), col = "grey75")
  points(d$kcal.per.g[i], density)  
}
```

The sum of the log of the likelihoods is therefore the log-likelihood of a
model.

```{r}
# By hand calculation
log_likelihoods <- dnorm(
  x = glm_m4$y, 
  mean = glm_m4$fitted.values, 
  sd = sigma_ml(glm_m4), 
  log = TRUE)
log_likelihoods

sum(log_likelihoods)

# Automatic calculation
logLik(glm_m4)

# Because residuals have mean 0, we can just use those instead of fitted values
dnorm(resid(glm_m4), mean = 0, sd = sigma_ml(glm_m4), log = TRUE) %>% sum
```

From the log-likelihood, we can compute deviance, AIC, and BIC.

```{r}
# as.numeric to get rid of df attribute
deviance_m4 <- as.numeric(-2 * logLik(glm_m4))
npars_m4 <- attr(logLik(glm_m4), "df")

deviance_m4
```

The information criteria are the deviance measures plus some penalty _k_ times
the number of parameters. For AIC, the penalty is _k_=2. For BIC, the penalty is
the log of the number of observations, _k_=log(_n_). The penalty adjusts the in-
sample deviance to get predicted value of the out-of-sample deviance. The model
with the lowest predicted out-of-sample deviance is the favored model.

```{r}
# Manual AIC vs automatic
aic_m4 <- deviance_m4 + 2 * npars_m4
aic_m4

AIC(glm_m4)  

# Manual BIC vs automatic  
bic_m4 <- deviance_m4 + log(nobs(glm_m4)) * npars_m4
bic_m4

BIC(glm_m4)
```




## Manual WAIC calculation

The above AIC example measured one log-likelihood based on one set of
parameters. Our Bayesian model reflects a distribution of parameter estimates,
and we sampled thousands of parameter estimates from the posterior distribution.
As a result, we have a distribution of log-likelihoods, and posterior
information criteria will incorporate information from the distribution of 
log-likelihoods.

The WAIC (Widely Applicable Information Criterion) is calculated using pointwise
log-likelihoods. Specifically, we calculate the log-pointwise-predictive density
(lppd). We observation has its own likelihood in each model, so we calculate
each observation's average likelihood across models and take the log.

RStanARM provides a `log_lik` to get an n_samples x n_observations matrix of
log-likelihoods.

```{r}
dim(log_lik(m4))

# log-pointwise-predictive density as the log of the average
# likelihood of each oberservation
each_lppd <- log_lik(m4) %>% exp %>% colMeans %>% log 
each_lppd
```

We also need a penalty term. Here, it's the effective number of parameters
(p_waic). Each observation contributes to the penalty term, using the variance
of its log-likelihoods.

```{r}
# effective number of parameters by taking variance of log-likelihood of each
# observation 
each_p_waic <- log_lik(m4) %>% apply(2, var)
each_p_waic
```

The WAIC is the difference in total lppd and total p_waic on the deviance scale.

```{r}
lppd <- sum(each_lppd)
lppd
p_waic <- sum(each_p_waic)
p_waic
waic <- -2 * (lppd - p_waic)
waic

# skip the summing step
each_waic <- -2 * (each_lppd - each_p_waic)
sum(each_waic)
```

But because each point contributes information to the lppd and p_waic
calculations, we can compute a standard error on these numbers.

```{r}
# standard error of the waics
se <- function(xs) sqrt(length(xs) * var(xs))
se(each_p_waic)
se(each_waic)
```

These calculations match the estimates from the loo package. The `loo::waic`
estimates use expected lppd (elpd) which already subtracts p_waic from lppd.
It's _expected_ because it is the log-likelihood of the in-sample data adjusted
with a penalty term. The resulting value is an expectation for out-of-sample
data.

```{r}
# manual elpd_waic
lppd - p_waic
se(each_lppd - each_p_waic)

loo::waic(m4)
```












## Getting WAIC for the models


Get the WAIC using the loo package.

I'm going to experiment with the
[many-models](http://r4ds.had.co.nz/many-models.html)/data-frame-of-models 
technique here. That means doing some as-of-2016 unconventional things with 
data-frames.

Basically, I make a data-frame with one row per model and then do things to each
model, storing the results of those operations as new columns in the data-frame.
Specifically, I calculate the WAIC of the Stan models and refit the models
using the classical technique so I can get an AIC value to compare to the WAIC
value. Then I will compute Akaike weights for based on ELPD values.

```{r}
# Get a data-frame summary from a loo::waic object
tidy_waic <- function(waic_fit) {
  to_keep <- c("waic", "se_waic", "elpd_waic", "se_elpd_waic", 
               "p_waic", "se_p_waic")
  ret <- waic_fit[to_keep]
  as_data_frame(ret)
}

# Create a data-frame with one-row per model
model_summary <- data_frame(StanFit = list(m1, m2, m3, m4)) %>% 
  mutate(
    # Use the formula from the model as a label
    Formula = map_chr(StanFit, ~ .x$formula %>% deparse),
    # Get the classical fit and deviance/AIC of classical fit
    ClassicalFit = map(StanFit, as_glm),
    Deviance = map_dbl(ClassicalFit, ~ logLik(.x) * -2),
    AIC = map_dbl(ClassicalFit, AIC),
    # Get the WAIC summary from the Stan fit of the model
    WAIC = map(StanFit, . %>% loo::waic() %>% tidy_waic)) %>% 
  # The WAIC is stored as a data-frame within each row. Unnest to promote the 
  # columns from the nested data-frame
  unnest(WAIC)

model_summary
```

The `rethinking::compare` function computes differences in WAIC and model
weights automatically, but won't work with RStanARM models so we compute those
by hand.

```{r}
model_summary <- model_summary %>% 
  mutate(diff_waic = min(waic) - waic,
         weight = exp(elpd_waic) / sum(exp(elpd_waic)))
```

We obtain the following table summarizing the models:

```{r}
# We need to exclude the list columns in order to print a formatted table 
model_summary %>% 
  select(-StanFit, -ClassicalFit) %>% 
  arrange(desc(weight)) %>% 
  knitr::kable(digits = 2) 
```

Remember what the book says about these weights:

> But what do these weights mean? There actually isn't a consensus about that.
> But here's Akaike's interpretation, which is common.
> 
> > A model's weight is an estimate of the probability that the model
> > will make the best predictions on new data, conditional on the set of models
> > considered.
> 
> Here’s the heuristic explanation. First, regard WAIC as the expected deviance
> of a model on future data. That is to say that WAIC gives us an estimate of
> E(D_test). Akaike weights convert these deviance values, which are
> log-likelihoods, to plain likelihoods and then standardize them all. This is
> just like Bayes’ theorem uses a sum in the denominator to standardize the 
> product of the likelihood and prior. Therefore the Akaike weights are analogous
> to posterior probabilities of models, conditional on expected future data.

McElreath goes on to note "However, given all the strong assumptions about
repeat sampling that go into calculating WAIC, you cannot take this heuristic
too seriously." The authors of the loo package themselves 
[no longer provide weights](https://github.com/stan-dev/loo/releases/tag/v0.1.5), 
citing a different limitation.

> In previous versions of **loo** model weights were also reported by `compare`.
> We have removed the weights because they were based only on the point estimate
> of the elpd values ignoring the uncertainty.

